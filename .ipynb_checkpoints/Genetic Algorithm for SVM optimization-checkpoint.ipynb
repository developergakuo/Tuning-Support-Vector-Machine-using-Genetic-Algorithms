{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validating Bug Prediction Models Assignment \n",
    "\n",
    "\n",
    "Below are the required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset operations (panda dataframes) \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# preprocessing (binarizer)\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#data balancing\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# accuracy metrics\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# feature selection \n",
    "from sklearn.datasets import load_digits\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector \n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "\n",
    "# single classifiers\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#deap imports\n",
    "import random\n",
    "import operator \n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "from deap import gp\n",
    "import array\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Read in CSV data and preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Reading in the csv file data with pandas into a dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "camel = \"./datasets/camel-1.6.csv\"\n",
    "jedit= \"./datasets/jedit-4.3.csv\"\n",
    "ant= \"./datasets/ant-1.7.csv\"\n",
    "log4j= \"./datasets/log4j-1.2.csv\"\n",
    "lucene= \"./datasets/lucene-2.4.csv\"\n",
    "data_sets = [('camel',camel), ('jedit',jedit), ('ant',ant), ('log4j',log4j),('lucene',lucene)]\n",
    "\n",
    "def preporcess_data(path):\n",
    "    data = pd.read_csv(path)\n",
    "    #Binarize the 'bug' column - '0' for No-bug and '1' for bug.\n",
    "    binarizer = preprocessing.Binarizer()\n",
    "    data['bug'] = binarizer.transform(data['bug'].values.reshape(-1,1))\n",
    "    #Separate the features (x) from the target column (y).\n",
    "    #Also, remove non-numeric fields that do not help in prediction.\n",
    "    data = data.drop(['name','version','name.1'], axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Data balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE \n",
    "Use synthetic Minority Oversampling Technique (SMOTE) and indicate the categorical columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(data):\n",
    "    x = data.drop(['bug'], axis=1)\n",
    "    y = data['bug']\n",
    "    X_balanced, y_balanced = SMOTE().fit_resample(x, y)\n",
    "    return X_balanced, y_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Feature selection\n",
    "In this task we aim to select the features that most influence the classification outcome. We aim to remove covariant features to reduce overfitting and the training cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate feature selection\n",
    "*Univariate feature selection works by selecting the best features based on univariate statistical tests. SelectPercentile (a type of Univariate feature selection) removes all but a user-specified highest scoring percentage of features.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_fetaures(x, y):\n",
    "    selection = SelectPercentile(chi2, percentile=80)\n",
    "    X_best_feature = selection.fit_transform(x, y)\n",
    "    y_best_feature = y\n",
    "    columns = np.asarray(x.columns.values)\n",
    "    support = np.asarray(selection.get_support())\n",
    "    columns_with_support = columns[support]\n",
    "    return columns_with_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 K-fold validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = 10\n",
    "\n",
    "#take in data and produce training and test sets pairs.\n",
    "def split_kfold_data(data):\n",
    "    x = data.drop(['bug'],axis=1)\n",
    "    y = data.bug\n",
    "    skf = StratifiedKFold(n_splits= FOLDS)\n",
    "    strata = skf.split(x,y)\n",
    "    train_and_test_folds = []\n",
    "    #produce train and test indexes on which to split data\n",
    "    for train, test in strata:\n",
    "        train_fold = data.iloc[train]\n",
    "        test_fold = data.iloc[test]\n",
    "        # balance the train_fold \n",
    "        balanced_x, balanced_y = balance_data(train_fold)\n",
    "        balanced_x = pd.DataFrame(balanced_x, columns=x.columns) \n",
    "        balanced_y = pd.DataFrame(balanced_y, columns=['bug']) \n",
    "        #select the best fearures from the balanced train_fold\n",
    "        selected_feature = select_fetaures(balanced_x, balanced_y)\n",
    "        X_best_feature = balanced_x[selected_feature]\n",
    "        y_best_feature = balanced_y.values.ravel()\n",
    "        #select only the selected features from the test_fold.\n",
    "        x_test_fold = test_fold[selected_feature]\n",
    "        y_test_fold = test_fold.bug\n",
    "        #collect each fold's training and test data.\n",
    "        train_and_test_folds.append((x_test_fold,y_test_fold,X_best_feature,y_best_feature))\n",
    "    return train_and_test_folds\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "# take an svm classifier and cross-validate (10 k-folds)\n",
    "# use matthews_corrcoef as a metric. Return the average mcc\n",
    "def cross_validate(clf,train_and_test_folds):\n",
    "    fold_metrics = []  \n",
    "    for x_test_fold, y_test_fold, X_best_feature, y_best_feature in train_and_test_folds:\n",
    "        # Fit svm\n",
    "        clf.fit(X_best_feature, y_best_feature)\n",
    "        # Check matthews_corrcoef on test set\n",
    "        pred = clf.predict(x_test_fold)\n",
    "        fold_metrics.append(matthews_corrcoef(y_test_fold, pred))\n",
    "    return sum(fold_metrics)/FOLDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a [gamma,c] array and produce its mcc accuracy as its optimization fitness\n",
    "def svm_optimization(params,train_and_test_folds):\n",
    "    gamma = params[0]\n",
    "    C = params[1]\n",
    "    clf = svm.SVC(gamma=gamma, C=C)\n",
    "    fitness = cross_validate(clf,train_and_test_folds)\n",
    "    return (fitness,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Geneteic algorithm optimization\n",
    "Here we use the DEAP framework to implement our evolutionary algorithm\n",
    "Create an individual creation function and population production function & Create evolution operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function to produce a random chromosome ([gamma,c]) with gamma (min - 0.0000010, max - 8) \n",
    "#and c (min-0.01, max -32000)\n",
    "#a chromosome is of type is given by the paramter icls = \"Individual\" defined above\n",
    "def initInd(icls,gamma_min,gamma_max, c_min,c_max):\n",
    "    gamma = random.uniform(gamma_min, gamma_max)\n",
    "    c= random.uniform(c_min, c_max)\n",
    "    ind = icls([gamma, c])\n",
    "    return ind\n",
    "\n",
    "GAMMA_MIN =  0.0000010\n",
    "GAMMA_MAX = 8\n",
    "C_MIN = 0.01\n",
    "C_MAX = 32000\n",
    "\n",
    "#this is a decorator function is required by the mutation operator to keep gamma and c within bounderies on mutation\n",
    "def checkBounds(gamma_min,gamma_max, c_min,c_max):\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kargs):\n",
    "            offspring = func(*args, **kargs)\n",
    "            for child in offspring:\n",
    "                    if child[0] > gamma_max:\n",
    "                        child[0] = gamma_max\n",
    "                    elif child[0] < gamma_min:\n",
    "                        child[0] = gamma_min\n",
    "                    if child[1] > c_max:\n",
    "                        child[1] = c_max\n",
    "                    elif child[1] < c_min:\n",
    "                        child[1] = c_min\n",
    "            return offspring\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Conduct the evolution optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-146-b6134d3adfa9>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-146-b6134d3adfa9>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    python -W ignore creator.py\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# a helper function to select the best individual to ever live during the evolution process\n",
    "def select_best_from_hall_of_fame(hall_of_fame_lst):\n",
    "    best,best_fitness = 0,0\n",
    "    for indiv,fitness in hall_of_fame_lst:\n",
    "        if best_fitness < fitness:\n",
    "            best = indiv\n",
    "            best_fitness = fitness\n",
    "    return best, best_fitness\n",
    "\n",
    "np.seterr(all=\"ignore\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# constants - cross-over probability , mutation probability, and number of generations respectively \n",
    "CXPB, MUTPB, NGEN = 0.5, 0.1, 300\n",
    "POPULATION_SIZE = 100\n",
    "MINIMUM_DELTA = 0.001\n",
    "\n",
    "def evolution(train_and_test_folds):\n",
    "    #initiate mutation objects and operators\n",
    "    #create a maximization optimization class (positive weight)\n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "    #create an individual class with a chromosome of type array ([gamma,c]) and a fitness \"FitnessMax\" defined above\n",
    "    creator.create(\"Individual\", array.array, typecode=\"d\", fitness=creator.FitnessMax)\n",
    "    #create a tool box to handle mutation operators\n",
    "    toolbox = base.Toolbox()\n",
    "    #register an operation to create individuals of type 'creator.Individual' using the function  'initInd'\n",
    "    toolbox.register(\"individual\", initInd, creator.Individual,GAMMA_MIN,GAMMA_MAX,C_MIN,C_MAX)\n",
    "    #register a function to repeatedly call the 'toolbox.individual' function to produce a list of individuals\n",
    "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "    #single point cross over operator\n",
    "    toolbox.register(\"mate\", tools.cxOnePoint)\n",
    "    #Mutation operator with uniform mutation within the gamma and c boundaries\n",
    "    toolbox.register(\"mutate\", tools.mutUniformInt, low=[0,0], up=[GAMMA_MAX,C_MAX], indpb=MUTPB)\n",
    "    #selection operator using tournaments of size 3\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "    #The fitness function used to conduct the selection\n",
    "    toolbox.register(\"evaluate\", svm_optimization,train_and_test_folds=train_and_test_folds)\n",
    "    #decorate the mutation operator with the tailormade \"checkBounds\" function above \n",
    "    # to keep gamma and c within its boundaries \n",
    "    toolbox.decorate(\"mutate\", checkBounds(GAMMA_MIN,GAMMA_MAX,C_MIN,C_MAX))\n",
    "    #create the population\n",
    "    pop = toolbox.population(n=POPULATION_SIZE)\n",
    "    #maintain a delta to check if the fitness is changing \n",
    "    delta = 0\n",
    "    previous_best_fitness = 0\n",
    "    current_best_fitness = 0\n",
    "    # list to collect the best individual and its fitness in each generation\n",
    "    Fittest_hall_of_fame = []\n",
    "    # Evaluate the entire population\n",
    "    fitnesses = map(toolbox.evaluate, pop)\n",
    "    #assign fitness to the individuals in the population\n",
    "    for ind, fit in zip(pop, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "    generation = 0\n",
    "    for g in range(NGEN):\n",
    "        changed = 0\n",
    "        #if the fitness does not change by more than the set minimum_delta, exit the evolution\n",
    "        if generation > 30:\n",
    "            if delta < MINIMUM_DELTA:\n",
    "                return select_best_from_hall_of_fame(Fittest_hall_of_fame)\n",
    "        # Select the next generation individuals. The population remains at a constant size\n",
    "        offspring = toolbox.select(pop, len(pop))\n",
    "        # Clone the selected individuals\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "        # Apply crossover and mutation on the offspring ([startAt:endBefore:skip])\n",
    "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if random.random() < CXPB:\n",
    "                #toolbox.mate produces the children in place\n",
    "                toolbox.mate(child1, child2)\n",
    "                #delete the fitness as the chromosome composition has changed\n",
    "                del child1.fitness.values\n",
    "                del child2.fitness.values\n",
    "        for mutant in offspring:\n",
    "            if random.random() < MUTPB:\n",
    "                toolbox.mutate(mutant)\n",
    "                del mutant.fitness.values\n",
    "        # Evaluate the individuals with an invalid fitness, those whose fitnesses we deleted\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "        #determine delta between previous generation and current generation\n",
    "        previous_best_fitness = current_best_fitness\n",
    "        current_best_fitest_indiv = toolbox.select(offspring,1)[0]\n",
    "        current_best_fitness, = svm_optimization(current_best_fitest_indiv,train_and_test_folds)\n",
    "        delta = abs(current_best_fitness - previous_best_fitness)\n",
    "        Fittest_hall_of_fame.append((current_best_fitest_indiv,current_best_fitness))\n",
    "        # The population is entirely replaced by the offspring\n",
    "        pop[:] = offspring\n",
    "        generation += 1\n",
    "    return  select_best_from_hall_of_fame(Fittest_hall_of_fame)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolution_helper(data_set):\n",
    "    name,path = data_set\n",
    "    data = preporcess_data(path)\n",
    "    train_and_test_folds = split_kfold_data(data)\n",
    "    #take the defauld [gamma,c] = ['auto',1.0] and see its fitness. Our aim is to improve on it\n",
    "    default_svm_fitness, = svm_optimization(['auto',1.0],train_and_test_folds)\n",
    "    print(name + \" default_svm_fitness\",default_svm_fitness)\n",
    "    evolution(train_and_test_folds)\n",
    "    return name, evolution(train_and_test_folds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel evolution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Pool(5)\n",
    "#evolution_helper(('camel',camel))\n",
    "print(p.map(evolution_helper, data_sets))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
